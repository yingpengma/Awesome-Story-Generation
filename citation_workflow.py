#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®ºæ–‡å¼•ç”¨æ•°ç®¡ç†å·¥å…· - å®Œæ•´å·¥ä½œæµ
æ•´åˆäº†è®ºæ–‡åŒæ­¥ã€å¼•ç”¨æ•°è·å–å’ŒREADMEæ›´æ–°çš„å®Œæ•´æµç¨‹
"""

import json
import re
import requests
from datetime import datetime
import sys
import time

class CitationWorkflow:
    def __init__(self):
        self.readme_file = 'README.md'
        self.citations_file = 'citations.json'
        self.api_delay = 1  # APIè¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œå›ºå®š1ç§’
        self.skip_recent_hours = 24  # è·³è¿‡24å°æ—¶å†…æ›´æ–°è¿‡çš„è®ºæ–‡
        
    def print_header(self, title, width=80):
        """æ‰“å°æ ¼å¼åŒ–çš„æ ‡é¢˜"""
        print("-"*width)
        print(f"{title:^{width}}")
        print("-"*width)
    
    def print_step(self, step_num, total_steps, description):
        """æ‰“å°æ­¥éª¤ä¿¡æ¯"""
        print(f"\n[æ­¥éª¤ {step_num}/{total_steps}] {description}")
    
    def extract_authors_from_readme(self, title):
        """ä»README.mdä¸­æå–æŒ‡å®šè®ºæ–‡çš„ä½œè€…ä¿¡æ¯"""
        try:
            with open(self.readme_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æŸ¥æ‰¾è®ºæ–‡æ ‡é¢˜çš„ä½ç½®
            title_pattern = r'\*\*' + re.escape(title) + r'\*\*'
            match = re.search(title_pattern, content)
            if not match:
                return "æœªæ‰¾åˆ°è®ºæ–‡æ ‡é¢˜"
            
            # ä»æ ‡é¢˜ä½ç½®å‘åæŸ¥æ‰¾ä½œè€…ä¿¡æ¯
            start_pos = match.end()
            line_content = content[start_pos:start_pos+500].split('\n')[0]  # åªåœ¨åŒä¸€è¡ŒæŸ¥æ‰¾
            
            # READMEæ ¼å¼: **æ ‡é¢˜** [[paper]](é“¾æ¥) [ä½œè€…ä¿¡æ¯] [å¯é€‰æ³¨é‡Š] [![citation badge
            # éœ€è¦è·³è¿‡ [[paper]](é“¾æ¥) éƒ¨åˆ†ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸æ˜¯[[å¼€å¤´çš„[å†…å®¹]
            
            # æ­£åˆ™è¡¨è¾¾å¼ï¼šè·³è¿‡æ‰€æœ‰[[...]](...) æ ¼å¼çš„é“¾æ¥ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªå•ç‹¬çš„[ä½œè€…ä¿¡æ¯]
            # æ ¼å¼ï¼š[[paper]](é“¾æ¥) æˆ– [[paper]](é“¾æ¥) [[code]](é“¾æ¥) åé¢çš„ [ä½œè€…ä¿¡æ¯]
            author_pattern = r'(?:\[\[[^\]]+\]\]\([^)]*\)\s*)+\[([^\]]+)\]'
            author_match = re.search(author_pattern, line_content)
            
            if author_match:
                authors = author_match.group(1).strip()
                # ç¡®ä¿è¿™æ˜¯ä½œè€…ä¿¡æ¯è€Œä¸æ˜¯å…¶ä»–å†…å®¹ï¼ˆå¦‚badgeæˆ–æ³¨é‡Šï¼‰
                if (not authors.startswith('!')  # ä¸æ˜¯badge
                    and 'http' not in authors.lower()  # ä¸æ˜¯é“¾æ¥
                    and len(authors) > 3):  # æœ‰å®é™…å†…å®¹
                    return authors
            
            return "æœªæ‰¾åˆ°ä½œè€…ä¿¡æ¯"
            
        except Exception as e:
            return f"æå–å¤±è´¥: {str(e)}"
    
    def extract_titles_from_md(self):
        """ä»README.mdä¸­æå–è®ºæ–‡æ ‡é¢˜"""
        try:
            with open(self.readme_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ›´æ–°åŒ¹é…æ¨¡å¼ä»¥é€‚åº”ä¸åŒæ ¼å¼çš„æ ‡é¢˜
            pattern = r'\*\*(.*?)\*\*\s*\['
            titles = re.findall(pattern, content)
            
            # æ¸…ç†æ ‡é¢˜ï¼ˆå»é™¤å¤šä½™çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ï¼‰
            paper_titles = [title.strip() for title in titles if not title.startswith('[') and len(title.strip()) > 10]
            
            print(f"âœ… æˆåŠŸä» {self.readme_file} ä¸­æå–äº† {len(paper_titles)} ç¯‡è®ºæ–‡")
            return paper_titles
            
        except FileNotFoundError:
            print(f'âŒ é”™è¯¯ï¼š{self.readme_file} æ–‡ä»¶ä¸å­˜åœ¨')
            sys.exit(1)
        except Exception as e:
            print(f'âŒ è¯»å– {self.readme_file} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}')
            sys.exit(1)
    
    def load_citations(self):
        """åŠ è½½citations.jsonæ–‡ä»¶"""
        try:
            with open(self.citations_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½ {self.citations_file}ï¼Œç°æœ‰ {len(data['papers'])} ç¯‡è®ºæ–‡")
            return data
        except FileNotFoundError:
            print(f'âš ï¸  {self.citations_file} æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶')
            return {'papers': {}}
        except json.JSONDecodeError:
            print(f'âŒ é”™è¯¯ï¼š{self.citations_file} æ ¼å¼ä¸æ­£ç¡®')
            sys.exit(1)
        except Exception as e:
            print(f'âŒ è¯»å– {self.citations_file} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}')
            sys.exit(1)
    
    def save_citations(self, data):
        """ä¿å­˜citationsæ•°æ®åˆ°æ–‡ä»¶"""
        try:
            with open(self.citations_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f'âŒ ä¿å­˜ {self.citations_file} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}')
            sys.exit(1)
    
    def sync_papers(self):
        """æ­¥éª¤1ï¼šåŒæ­¥README.mdå’Œcitations.jsonä¸­çš„è®ºæ–‡åˆ—è¡¨"""
        self.print_step(1, 3, "åŒæ­¥è®ºæ–‡åˆ—è¡¨")
        
        # ä»README.mdæå–è®ºæ–‡æ ‡é¢˜
        md_titles = self.extract_titles_from_md()
        
        # åŠ è½½å½“å‰çš„citationsæ•°æ®
        citations_data = self.load_citations()
        
        # è·å–å½“å‰æ—¥æœŸ
        current_date = datetime.now().strftime('%Y-%m-%d-%H:%M:%S')
        
        # åˆ›å»ºæ–°çš„citationsæ•°æ®ï¼ŒåªåŒ…å«mdä¸­å­˜åœ¨çš„è®ºæ–‡
        new_citations = {'papers': {}}
        added_count = 0
        
        print("\nğŸ“‹ å¼€å§‹åŒæ­¥è®ºæ–‡åˆ—è¡¨...")
        
        # åŒæ­¥å¤„ç†
        for title in md_titles:
            if title in citations_data.get('papers', {}):
                # ä¿ç•™å·²æœ‰è®ºæ–‡çš„å¼•ç”¨æ•°æ®
                new_citations['papers'][title] = citations_data['papers'][title]
            else:
                # æ·»åŠ æ–°è®ºæ–‡
                new_citations['papers'][title] = {
                    'title': title,  # æ·»åŠ æ ‡é¢˜å­—æ®µ
                    'citations': 0,
                    'last_updated': '1970-01-01-00:00:00'  # è®¾ç½®å¾ˆä¹…ä»¥å‰çš„æ—¶é—´ï¼Œç¡®ä¿æ–°è®ºæ–‡ä¼šè¢«ç«‹å³æ›´æ–°
                }
                print(f"â• æ·»åŠ æ–°è®ºæ–‡ï¼š{title}")
                added_count += 1
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è®ºæ–‡åœ¨README.mdä¸­ä¸å­˜åœ¨ï¼ˆåªä»æ•°æ®ä¸­ç§»é™¤ï¼Œä¸ä¿®æ”¹READMEæ–‡ä»¶ï¼‰
        old_papers = set(citations_data.get('papers', {}).keys())
        removed_papers = old_papers - set(md_titles)
        if removed_papers:
            print(f"\nğŸ“‹ ä»¥ä¸‹ {len(removed_papers)} ç¯‡è®ºæ–‡åœ¨README.mdä¸­ä¸å­˜åœ¨ï¼Œå°†ä»æ•°æ®ä¸­ç§»é™¤ï¼ˆä¸ä¿®æ”¹READMEæ–‡ä»¶ï¼‰ï¼š")
            for title in removed_papers:
                print(f"   - {title}")
        
        # ä¿å­˜æ›´æ–°åçš„æ•°æ®
        self.save_citations(new_citations)
        
        print(f"\nâœ… è®ºæ–‡åˆ—è¡¨åŒæ­¥å®Œæˆï¼")
        print(f"   ğŸ“Š å½“å‰æ€»è®¡ï¼š{len(new_citations['papers'])} ç¯‡è®ºæ–‡")
        print(f"   â• æ–°å¢ï¼š{added_count} ç¯‡")
        print(f"   ğŸ“‹ æ•°æ®æ¸…ç†ï¼š{len(removed_papers)} ç¯‡")
        
        return new_citations
    
    def is_recently_updated(self, last_updated_str):
        """æ£€æŸ¥è®ºæ–‡æ˜¯å¦åœ¨æœ€è¿‘24å°æ—¶å†…æ›´æ–°è¿‡"""
        try:
            from datetime import timedelta
            
            # è§£ææœ€åæ›´æ–°æ—¶é—´
            last_updated = datetime.strptime(last_updated_str, '%Y-%m-%d-%H:%M:%S')
            
            # è®¡ç®—æ—¶é—´å·®
            time_diff = datetime.now() - last_updated
            
            # å¦‚æœå°äºè®¾å®šçš„è·³è¿‡æ—¶é—´ï¼Œè¿”å›True
            return time_diff < timedelta(hours=self.skip_recent_hours)
            
        except (ValueError, TypeError):
            # å¦‚æœæ—¶é—´æ ¼å¼è§£æå¤±è´¥ï¼Œè®¤ä¸ºéœ€è¦æ›´æ–°
            return False
    
    def update_paper_citation(self, title, paper_data, retry_limit=3):
        """è·å–å•ç¯‡è®ºæ–‡çš„å¼•ç”¨æ•°ï¼Œå¹¶ç«‹å³æ›´æ–°JSONæ–‡ä»¶"""
        base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
        headers = {
            "Content-Type": "application/json",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        old_citation = paper_data["papers"][title]["citations"]
        search_title = paper_data["papers"][title]["title"]  # ä½¿ç”¨æ ‡é¢˜å­—æ®µæœç´¢
        print(f"\nğŸ“– è®ºæ–‡: '{title}'")
        if search_title != title:
            print(f"ğŸ” é€šè¿‡æ ‡é¢˜æœç´¢: '{search_title}'")
        
        for attempt in range(retry_limit):
            try:
                # ä½¿ç”¨searchæ¥å£æŸ¥è¯¢è®ºæ–‡
                params = {
                    "query": search_title,  # ä½¿ç”¨æ ‡é¢˜æœç´¢
                    "fields": "title,citationCount,url,authors",  # æ·»åŠ ä½œè€…ä¿¡æ¯
                    "limit": 10  # è·å–å‡ ä¸ªåŒ¹é…ç»“æœä»¥æé«˜æ‰¾åˆ°çš„æ¦‚ç‡
                }
                
                response = requests.get(base_url, params=params, headers=headers)
                
                # ç‰¹æ®Šå¤„ç†429é”™è¯¯ï¼Œç›´æ¥è·³è¿‡ä¸æ˜¾ç¤º
                if response.status_code == 429:
                    if attempt < retry_limit - 1:
                        print(f"   âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•: è¯·æ±‚é¢‘ç‡é™åˆ¶ï¼Œ1ç§’åé‡è¯•...")
                        time.sleep(self.api_delay)  # å›ºå®š1ç§’ç­‰å¾…æ—¶é—´
                        continue
                    else:
                        print(f"   âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡æ­¤è®ºæ–‡")
                        return False, "å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°"
                
                response.raise_for_status()
                data = response.json()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
                if not data.get("data") or len(data["data"]) == 0:
                    if attempt < retry_limit - 1:
                        print(f"   âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•: APIè¿”å›ç©ºç»“æœï¼Œé‡è¯•...")
                        time.sleep(self.api_delay)
                        continue
                    else:
                        print(f"   âŒ æœªæ‰¾åˆ°: APIè¿”å›ç©ºç»“æœ")
                        return False, "APIè¿”å›ç©ºç»“æœ"
                
                # å°è¯•ä»ç»“æœä¸­æ‰¾åˆ°å®Œå…¨åŒ¹é…çš„è®ºæ–‡æ ‡é¢˜
                found = False
                for paper in data["data"]:
                    if 'title' not in paper or 'citationCount' not in paper:
                        continue
                    
                    # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦å®Œå…¨åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
                    if paper['title'].lower() == search_title.lower():
                        new_citation = paper['citationCount']
                        
                        # æ˜¾ç¤ºå¼•ç”¨å˜åŒ–
                        change = new_citation - old_citation
                        if change > 0:
                            change_symbol = f"ğŸ“ˆ +{change}"
                        elif change < 0:
                            change_symbol = f"ğŸ“‰ {change}"
                        else:
                            change_symbol = "ğŸ“Š æ— å˜åŒ–"
                        
                        print(f"   âœ… æ›´æ–°æˆåŠŸ: {old_citation} â†’ {new_citation} ({change_symbol})")
                        if 'url' in paper and paper['url']:
                            print(f"   ğŸ”— è®ºæ–‡é“¾æ¥: {paper['url']}")
                        
                        # ç«‹å³æ›´æ–°æ•°æ®
                        paper_data["papers"][title]["citations"] = new_citation
                        current_time = datetime.now().strftime("%Y-%m-%d-%H:%M:%S")
                        paper_data["papers"][title]["last_updated"] = current_time
                        
                        # ç«‹å³ä¿å­˜åˆ°æ–‡ä»¶
                        self.save_citations(paper_data)
                        
                        found = True
                        return True, new_citation
                
                if not found:
                    # ä¸é‡è¯•ï¼Œæ˜¾ç¤ºæœ€ä½³åŒ¹é…ç»“æœå’Œä½œè€…ä¿¡æ¯
                    print(f"   âŒ æœªæ‰¾åˆ°å®Œå…¨åŒ¹é…çš„æ ‡é¢˜")
                    
                    if data.get("data") and len(data["data"]) > 0:
                        # æ‰¾åˆ°ç›¸ä¼¼åº¦æœ€é«˜çš„ç»“æœï¼ˆå–ç¬¬ä¸€ä¸ªï¼‰
                        best_match = data["data"][0]
                        print(f"   ğŸ” æœ€ä½³åŒ¹é…ç»“æœ:")
                        print(f"      APIæ ‡é¢˜: '{best_match.get('title', 'æ— æ ‡é¢˜')}'")
                        print(f"      å½“å‰æœç´¢: '{search_title}'")
                        
                        # æ˜¾ç¤ºä½œè€…ä¿¡æ¯
                        readme_authors = self.extract_authors_from_readme(title)
                        print(f"      READMEä½œè€…: {readme_authors}")
                        
                        api_authors = "æ— ä½œè€…ä¿¡æ¯"
                        if 'authors' in best_match and best_match['authors']:
                            author_names = [author.get('name', 'æœªçŸ¥') for author in best_match['authors'][:5]]  # åªæ˜¾ç¤ºå‰5ä¸ªä½œè€…
                            api_authors = ', '.join(author_names)
                            if len(best_match['authors']) > 5:
                                api_authors += f" ç­‰{len(best_match['authors'])}äºº"
                        print(f"      APIä½œè€…: {api_authors}")
                        
                        print(f"   ğŸ“ å¦‚ç¡®è®¤æ˜¯åŒä¸€ç¯‡è®ºæ–‡ï¼Œè¯·æ‰‹åŠ¨ä¿®æ”¹citations.jsonä¸­è¯¥è®ºæ–‡çš„titleå­—æ®µ")
                    
                    return False, "æ ‡é¢˜ä¸å®Œå…¨åŒ¹é…"
                        
            except requests.exceptions.HTTPError as e:
                if "429" in str(e):  # é¢‘ç‡é™åˆ¶é”™è¯¯ï¼Œé‡è¯•
                    if attempt < retry_limit - 1:
                        print(f"   âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•: è¯·æ±‚é¢‘ç‡é™åˆ¶ï¼Œ1ç§’åé‡è¯•...")
                        time.sleep(self.api_delay)
                        continue
                    else:
                        print(f"   âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡æ­¤è®ºæ–‡")
                        return False, "å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°"
                else:
                    if attempt < retry_limit - 1:
                        print(f"   âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•: HTTPé”™è¯¯({e})ï¼Œé‡è¯•...")
                        time.sleep(self.api_delay)
                        continue
                    else:
                        print(f"   âŒ APIè¯·æ±‚é”™è¯¯: {e}")
                        return False, f"HTTPé”™è¯¯"
            except requests.exceptions.RequestException as e:
                if attempt < retry_limit - 1:
                    print(f"   âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•: è¯·æ±‚å¼‚å¸¸({e})ï¼Œé‡è¯•...")
                    time.sleep(self.api_delay)
                    continue
                else:
                    print(f"   âŒ APIè¯·æ±‚é”™è¯¯: {e}")
                    return False, "è¯·æ±‚å¼‚å¸¸"
            
            # è¯·æ±‚æˆåŠŸåç­‰å¾…ä¸€æ®µæ—¶é—´é¿å…é¢‘ç‡é™åˆ¶
            if attempt < retry_limit - 1:
                time.sleep(self.api_delay)
        
        return False, "è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°"
    
    def get_citations(self, citations_data):
        """æ­¥éª¤2ï¼šè·å–è®ºæ–‡å¼•ç”¨æ•°"""
        self.print_step(2, 3, "è·å–æœ€æ–°å¼•ç”¨æ•°")
        
        # å¯¹è®ºæ–‡æ ‡é¢˜è¿›è¡Œæ’åºï¼ŒæŒ‰æ›´æ–°æ—¥æœŸå‡åºï¼ˆæœ€ä¹…æœªæ›´æ–°çš„æ’åœ¨å‰é¢ï¼‰
        paper_info = []
        for title, info in citations_data["papers"].items():
            # å¯¹äºæ²¡æœ‰last_updatedå­—æ®µçš„è®ºæ–‡ï¼Œè®¾ç½®ä¸€ä¸ªé»˜è®¤å€¼
            last_updated = info.get("last_updated", "1970-01-01-00:00:00")
            paper_info.append({
                "title": title,
                "citations": info["citations"],
                "last_updated": last_updated
            })
        
        # åªæŒ‰æ›´æ–°æ—¥æœŸæ’åºï¼ˆæœ€æ—§çš„åœ¨å‰ï¼Œæœ€æ–°çš„åœ¨åï¼‰
        sorted_papers = sorted(paper_info, key=lambda x: x["last_updated"])
        
        # æå–æ’åºåçš„æ ‡é¢˜åˆ—è¡¨
        paper_titles_ordered = [paper["title"] for paper in sorted_papers]
        
        # è¾“å‡ºæ’åºä¿¡æ¯
        print(f"\nğŸ“Š è®ºæ–‡æ›´æ–°ä¼˜å…ˆçº§æ’åºï¼ˆå…±{len(sorted_papers)}ç¯‡ï¼ŒæŒ‰æœ€åæ›´æ–°æ—¶é—´å‡åºï¼‰:")
        for i, paper in enumerate(sorted_papers[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   {i+1}. å¼•ç”¨: {paper['citations']}, æœ€åæ›´æ–°: {paper['last_updated']}")
        if len(sorted_papers) > 5:
            print(f"   ... è¿˜æœ‰ {len(sorted_papers)-5} ç¯‡è®ºæ–‡")
        
        # åˆ›å»ºä¸€ä¸ªæ–°çš„æœ‰åºå­—å…¸ï¼Œä¿ç•™åŸå§‹æ•°æ®ç»“æ„ä½†æŒ‰æ–°é¡ºåºæ’åˆ—
        sorted_data = {"papers": {}}
        for title in paper_titles_ordered:
            sorted_data["papers"][title] = citations_data["papers"][title]
        
        # ä¿å­˜æ’åºåçš„æ•°æ®åˆ°åŸå§‹æ–‡ä»¶
        self.save_citations(sorted_data)
        
        # é€ä¸ªæ›´æ–°å¼•ç”¨æ•°å¹¶ç«‹å³ä¿å­˜
        updated_papers = []
        skipped_papers = []
        skipped_recent = []
        
        need_update_count = 0
        # å…ˆç»Ÿè®¡éœ€è¦æ›´æ–°çš„è®ºæ–‡æ•°é‡
        for title in paper_titles_ordered:
            last_updated = sorted_data["papers"][title].get("last_updated", "1970-01-01-00:00:00")
            if not self.is_recently_updated(last_updated):
                need_update_count += 1
        
        print(f"ğŸ“Š éœ€è¦æ›´æ–°: {need_update_count} ç¯‡ï¼Œè·³è¿‡: {len(paper_titles_ordered) - need_update_count} ç¯‡ï¼ˆ24å°æ—¶å†…å·²æ›´æ–°ï¼‰")
        
        current_update_index = 0
        for index, title in enumerate(paper_titles_ordered):
            # æ£€æŸ¥æ˜¯å¦åœ¨24å°æ—¶å†…æ›´æ–°è¿‡
            last_updated = sorted_data["papers"][title].get("last_updated", "1970-01-01-00:00:00")
            if self.is_recently_updated(last_updated):
                skipped_recent.append({
                    "title": title,
                    "last_updated": last_updated
                })
                continue
            
            current_update_index += 1
            print(f"\nğŸ”„ å¤„ç†è®ºæ–‡ {current_update_index}/{need_update_count}")
            
            success, result = self.update_paper_citation(title, sorted_data)
            
            if success:
                old_citations = sorted_data["papers"][title]["citations"] - (result if isinstance(result, int) else 0)
                updated_papers.append({
                    "title": title,
                    "old": old_citations,
                    "new": sorted_data["papers"][title]["citations"],
                    "change": sorted_data["papers"][title]["citations"] - old_citations,
                    "updated_time": sorted_data["papers"][title]["last_updated"]
                })
            else:
                skipped_papers.append({
                    "title": title,
                    "reason": result
                })
            
            # åœ¨è¯·æ±‚é—´æ·»åŠ å»¶è¿Ÿ
            if index < len(paper_titles_ordered) - 1:
                time.sleep(self.api_delay)
        
        # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
        print(f"\nâœ… æˆåŠŸæ›´æ–°: {len(updated_papers)} ({(len(updated_papers)/len(paper_titles_ordered)*100):.1f}%)")
        print(f"â­ï¸ è·³è¿‡ï¼ˆ24å°æ—¶å†…å·²æ›´æ–°ï¼‰: {len(skipped_recent)} ({(len(skipped_recent)/len(paper_titles_ordered)*100):.1f}%)")

        if updated_papers:
            print(f"ğŸ“ˆ æˆåŠŸæ›´æ–°çš„è®ºæ–‡ ({len(updated_papers)}ç¯‡):")
            for i, paper in enumerate(updated_papers[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
                change_symbol = "ğŸ“ˆ" if paper['change'] > 0 else "ğŸ“‰" if paper['change'] < 0 else "ğŸ“Š"
                print(f"   {i+1}. '{paper['title'][:50]}{'...' if len(paper['title']) > 50 else ''}'")
                print(f"      å¼•ç”¨å˜åŒ–: {paper['old']} â†’ {paper['new']} ({change_symbol}{paper['change']:+d})")
            if len(updated_papers) > 10:
                print(f"   ... è¿˜æœ‰ {len(updated_papers)-10} ç¯‡æˆåŠŸæ›´æ–°")
        
        
        if skipped_papers:
            print(f"âŒ æœªèƒ½æ›´æ–°çš„è®ºæ–‡ ({len(skipped_papers)}ç¯‡):")
            for i, paper in enumerate(skipped_papers[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   {i+1}. '{paper['title'][:50]}{'...' if len(paper['title']) > 50 else ''}'")
                print(f"      åŸå› : {paper['reason']}")
            if len(skipped_papers) > 5:
                print(f"   ... è¿˜æœ‰ {len(skipped_papers)-5} ç¯‡æ›´æ–°å¤±è´¥")
        
        return sorted_data, updated_papers
    
    def update_readme_citations(self, citations_data, updated_papers_list):
        """æ­¥éª¤3ï¼šæ›´æ–°README.mdä¸­çš„citation badgesï¼ˆä¸¥æ ¼ä¿æŠ¤ï¼Œåªæ›´æ–°badgesï¼‰"""
        self.print_step(3, 3, "æ›´æ–°README.mdä¸­çš„å¼•ç”¨æ ‡ç­¾")
        
        # å¦‚æœæ²¡æœ‰è®ºæ–‡éœ€è¦æ›´æ–°README
        if not updated_papers_list:
            return True
        
        try:
            with open(self.readme_file, 'r', encoding='utf-8') as f:
                original_content = f.read()
                md_content = original_content  # ä¿ç•™åŸå§‹å†…å®¹å‰¯æœ¬
        except Exception as e:
            print(f'âŒ è¯»å– {self.readme_file} å¤±è´¥: {str(e)}')
            return False
        
        # åªæ›´æ–°å®é™…åœ¨æ­¥éª¤2ä¸­æˆåŠŸæ›´æ–°çš„è®ºæ–‡
        papers_to_update = [paper["title"] for paper in updated_papers_list]
        print(f"ğŸ“„ å¼€å§‹æ›´æ–° {len(papers_to_update)} ç¯‡è®ºæ–‡çš„citation badges...")
        
        updated_count = 0
        error_count = 0
        
        # åªéå†éœ€è¦æ›´æ–°çš„è®ºæ–‡
        for paper_title in papers_to_update:
            try:
                paper_info = citations_data['papers'][paper_title]
                
                # åœ¨markdownä¸­æŸ¥æ‰¾å®Œå…¨åŒ¹é…çš„è®ºæ–‡æ ‡é¢˜
                matches = [m.start() for m in re.finditer(r'\*\*' + re.escape(paper_title) + r'\*\*', md_content)]
                
                # æ£€æŸ¥åŒ¹é…æ•°é‡
                if len(matches) == 0:
                    print(f"   âš ï¸ æœªæ‰¾åˆ°è®ºæ–‡æ ‡é¢˜çš„åŒ¹é…: {paper_title}")
                    error_count += 1
                    continue
                elif len(matches) > 1:
                    print(f"   âš ï¸ æ‰¾åˆ°å¤šä¸ªè®ºæ–‡æ ‡é¢˜çš„åŒ¹é…: {paper_title}")
                    error_count += 1
                    continue
                
                # æ‰¾åˆ°åŒ¹é…ä½ç½®åçš„citation badge
                match_pos = matches[0]
                badge_pattern = r'\[!\[\]\(https://img\.shields\.io/badge/citation-\d+-blue\)\]\(\)'
                badge_match = re.search(badge_pattern, md_content[match_pos:])
                
                if not badge_match:
                    print(f"   âš ï¸ æœªæ‰¾åˆ°è®ºæ–‡çš„citation badge: {paper_title}")
                    error_count += 1
                    continue
                
                # æ›´æ–°citationæ•°é‡
                old_badge = badge_match.group(0)
                new_badge = f'[![](https://img.shields.io/badge/citation-{paper_info["citations"]}-blue)]()'        
                
                # åªæ›¿æ¢å½“å‰è®ºæ–‡ä½ç½®åçš„ç¬¬ä¸€ä¸ªbadge
                before_match = md_content[:match_pos + badge_match.start()]
                after_match = md_content[match_pos + badge_match.end():]
                md_content = before_match + new_badge + after_match
                
                print(f"   âœ… æˆåŠŸæ›´æ–°: '{paper_title[:50]}{'...' if len(paper_title) > 50 else ''}' â†’ {paper_info['citations']} å¼•ç”¨")
                updated_count += 1
                
            except Exception as e:
                print(f"   âŒ å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {paper_title} - {str(e)}")
                error_count += 1
                continue
        
        # éªŒè¯å†…å®¹å®Œæ•´æ€§ï¼šæ£€æŸ¥é™¤äº†citation badgeså¤–æ˜¯å¦æœ‰å…¶ä»–å˜åŒ–
        print(f"\nğŸ” éªŒè¯å†…å®¹å®Œæ•´æ€§...")
        
        # ç§»é™¤æ‰€æœ‰citation badgesè¿›è¡Œå¯¹æ¯”
        original_no_badges = re.sub(r'\[!\[\]\(https://img\.shields\.io/badge/citation-\d+-blue\)\]\(\)', '', original_content)
        updated_no_badges = re.sub(r'\[!\[\]\(https://img\.shields\.io/badge/citation-\d+-blue\)\]\(\)', '', md_content)
        
        if original_no_badges != updated_no_badges:
            print(f"âŒ æ£€æµ‹åˆ°é™¤citation badgeså¤–çš„å…¶ä»–å†…å®¹å˜åŒ–ï¼Œæ‹’ç»ä¿å­˜ä»¥ä¿æŠ¤READMEå®Œæ•´æ€§")
            return False
        
        # ä¿å­˜æ›´æ–°åçš„README.md
        try:
            with open(self.readme_file, 'w', encoding='utf-8') as f:
                f.write(md_content)
            print(f"âœ… å†…å®¹å®Œæ•´æ€§éªŒè¯é€šè¿‡")
            print(f"ğŸ’¾ {self.readme_file} æ›´æ–°å®Œæˆï¼")
            print(f"   âœ… æˆåŠŸæ›´æ–°: {updated_count} ä¸ªcitation badges")
            if error_count > 0:
                print(f"   âš ï¸ æ›´æ–°å¤±è´¥: {error_count} ä¸ª")
            return True
        except Exception as e:
            print(f'âŒ ä¿å­˜ {self.readme_file} å¤±è´¥: {str(e)}')
            return False
    
    def run_workflow(self):
        """è¿è¡Œå®Œæ•´çš„å¼•ç”¨æ•°ç®¡ç†å·¥ä½œæµ"""
        print(f"â° å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # æ­¥éª¤1: åŒæ­¥è®ºæ–‡åˆ—è¡¨
            citations_data = self.sync_papers()
            
            # æ­¥éª¤2: è·å–å¼•ç”¨æ•°
            updated_data, updated_papers_list = self.get_citations(citations_data)
            
            # æ­¥éª¤3: æ›´æ–°README
            readme_success = self.update_readme_citations(updated_data, updated_papers_list)
            
            # æœ€ç»ˆæŠ¥å‘Š
            
            print(f"\n{'âœ…' if readme_success else 'âŒ'} æ­¥éª¤3 - READMEæ›´æ–°: {'å®Œæˆ' if readme_success else 'å¤±è´¥'}")
            print(f"â° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        except KeyboardInterrupt:
            print(f"\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­äº†å·¥ä½œæµç¨‹")
            print(f"ğŸ’¾ å½“å‰è¿›åº¦å·²ä¿å­˜åˆ° {self.citations_file}")
        except Exception as e:
            print(f"\n\nâŒ å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}")
            sys.exit(1)

def main():
    """ä¸»å‡½æ•°"""
    workflow = CitationWorkflow()
    workflow.run_workflow()

if __name__ == "__main__":
    main() 