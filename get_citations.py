import json
import requests
from datetime import datetime

def update_paper_citation(title, paper_data, file_path, retry_limit=3):
    """è·å–å•ç¯‡è®ºæ–‡çš„å¼•ç”¨æ•°ï¼Œå¹¶ç«‹å³æ›´æ–°JSONæ–‡ä»¶"""
    base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
    headers = {
        "Content-Type": "application/json",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    old_citation = paper_data["papers"][title]["citations"]
    print(f"\nè®ºæ–‡: '{title}'")
    
    for attempt in range(retry_limit):
        try:
            # ä½¿ç”¨searchæ¥å£æŸ¥è¯¢è®ºæ–‡
            params = {
                "query": title,
                "fields": "title,citationCount,url",
                "limit": 10  # è·å–å‡ ä¸ªåŒ¹é…ç»“æœä»¥æé«˜æ‰¾åˆ°çš„æ¦‚ç‡
            }
            
            response = requests.get(base_url, params=params, headers=headers)
            
            # ç‰¹æ®Šå¤„ç†429é”™è¯¯ï¼Œç›´æ¥è·³è¿‡ä¸æ˜¾ç¤º
            if response.status_code == 429:
                if attempt < retry_limit - 1:
                    print(f"  âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•: è·³è¿‡å¹¶é‡è¯•...")
                    continue
                else:
                    print(f"  âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡æ­¤è®ºæ–‡")
                    return False, "å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°"
            
            response.raise_for_status()
            data = response.json()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
            if not data.get("data") or len(data["data"]) == 0:
                if attempt < retry_limit - 1:
                    print(f"  âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•: APIè¿”å›ç©ºç»“æœï¼Œé‡è¯•...")
                    continue
                else:
                    print(f"  âŒ æœªæ‰¾åˆ°: APIè¿”å›ç©ºç»“æœ")
                    return False, "APIè¿”å›ç©ºç»“æœ"
            
            # å°è¯•ä»ç»“æœä¸­æ‰¾åˆ°å®Œå…¨åŒ¹é…çš„è®ºæ–‡æ ‡é¢˜
            found = False
            for paper in data["data"]:
                if 'title' not in paper or 'citationCount' not in paper:
                    continue
                
                # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦å®Œå…¨åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
                if paper['title'].lower() == title.lower():
                    new_citation = paper['citationCount']
                    
                    # æ˜¾ç¤ºå¼•ç”¨å˜åŒ–
                    change = new_citation - old_citation
                    change_symbol = "+" if change > 0 else ""
                    
                    print(f"  âœ… æ›´æ–°æˆåŠŸ: {old_citation} â†’ {new_citation} ({change_symbol}{change})")
                    if 'url' in paper and paper['url']:
                        print(f"  ğŸ“„ è®ºæ–‡é“¾æ¥: {paper['url']}")
                    
                    # ç«‹å³æ›´æ–°æ•°æ®
                    paper_data["papers"][title]["citations"] = new_citation
                    current_time = datetime.now().strftime("%Y-%m-%d-%H:%M:%S")
                    paper_data["papers"][title]["last_updated"] = current_time
                    
                    # ç«‹å³ä¿å­˜åˆ°æ–‡ä»¶
                    with open(file_path, 'w', encoding='utf-8') as file:
                        json.dump(paper_data, file, indent=2, ensure_ascii=False)
                    print(f"  ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶")
                    
                    found = True
                    return True, new_citation
            
            if not found:
                if attempt < retry_limit - 1:
                    print(f"  âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•: æ ‡é¢˜ä¸å®Œå…¨åŒ¹é…ï¼Œé‡è¯•...")
                    continue
                else:
                    print(f"  âŒ æœªæ‰¾åˆ°: æ ‡é¢˜ä¸å®Œå…¨åŒ¹é…")
                    print(f"  ğŸ“ å¤‡æ³¨: æ‰¾åˆ°äº†{len(data['data'])}ä¸ªç»“æœï¼Œä½†æ²¡æœ‰æ ‡é¢˜å®Œå…¨åŒ¹é…çš„")
                    return False, "æ ‡é¢˜ä¸å®Œå…¨åŒ¹é…"
                
        except requests.exceptions.HTTPError as e:
            if "429" in str(e):  # ä¸æ˜¾ç¤º429é”™è¯¯
                if attempt < retry_limit - 1:
                    print(f"  âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•: è·³è¿‡å¹¶é‡è¯•...")
                    continue
                else:
                    print(f"  âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡æ­¤è®ºæ–‡")
                    return False, "å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°"
            else:
                if attempt < retry_limit - 1:
                    print(f"  âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•: HTTPé”™è¯¯ï¼Œé‡è¯•...")
                    continue
                else:
                    print(f"  âŒ APIè¯·æ±‚é”™è¯¯")
                    return False, f"HTTPé”™è¯¯"
        except requests.exceptions.RequestException:
            if attempt < retry_limit - 1:
                print(f"  âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•: è¯·æ±‚å¼‚å¸¸ï¼Œé‡è¯•...")
                continue
            else:
                print(f"  âŒ APIè¯·æ±‚é”™è¯¯")
                return False, "è¯·æ±‚å¼‚å¸¸"
    
    return False, "è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°"

def update_citations_file(file_path):
    """æ›´æ–°citations.jsonæ–‡ä»¶ä¸­çš„å¼•ç”¨æ•°å¹¶è¯¦ç»†è®°å½•æ•´ä¸ªè¿‡ç¨‹"""
    # è¯»å–ç°æœ‰çš„citations.jsonæ–‡ä»¶
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            data = json.load(file)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"è¯»å–æ–‡ä»¶é”™è¯¯: {e}")
        return
    
    # å¯¹è®ºæ–‡æ ‡é¢˜è¿›è¡Œæ’åºï¼ŒåªæŒ‰æ›´æ–°æ—¥æœŸå‡åºï¼ˆæœ€ä¹…æœªæ›´æ–°çš„æ’åœ¨å‰é¢ï¼‰
    paper_info = []
    for title, info in data["papers"].items():
        # å¯¹äºæ²¡æœ‰last_updatedå­—æ®µçš„è®ºæ–‡ï¼Œè®¾ç½®ä¸€ä¸ªé»˜è®¤å€¼
        last_updated = info.get("last_updated", "1970-01-01-00:00:00")
        paper_info.append({
            "title": title,
            "citations": info["citations"],
            "last_updated": last_updated
        })
    
    # åªæŒ‰æ›´æ–°æ—¥æœŸæ’åºï¼ˆæœ€æ—§çš„åœ¨å‰ï¼Œæœ€æ–°çš„åœ¨åï¼‰
    sorted_papers = sorted(paper_info, key=lambda x: x["last_updated"])
    
    # æå–æ’åºåçš„æ ‡é¢˜åˆ—è¡¨
    paper_titles_ordered = [paper["title"] for paper in sorted_papers]
    
    # è¾“å‡ºæ’åºä¿¡æ¯
    print("\n" + "="*80)
    print("è®ºæ–‡æ’åºä¿¡æ¯ï¼ˆæŒ‰æœ€åæ›´æ–°æ—¶é—´å‡åºæ’åˆ—ï¼Œæœ€ä¹…æœªæ›´æ–°çš„æ’åœ¨å‰é¢ï¼‰")
    print("="*80)
    for i, paper in enumerate(sorted_papers):
        print(f"{i+1}. '{paper['title']}' - å¼•ç”¨: {paper['citations']}, æœ€åæ›´æ–°: {paper['last_updated']}")
    
    # å°†æ’åºç»“æœä¿å­˜åˆ°åŸå§‹æ–‡ä»¶
    print("\n" + "="*80)
    print("æ’åºå®Œæˆï¼Œä¿å­˜æ’åºåçš„æ•°æ®åˆ°åŸå§‹æ–‡ä»¶")
    print("="*80)
    
    # åˆ›å»ºä¸€ä¸ªæ–°çš„æœ‰åºå­—å…¸ï¼Œä¿ç•™åŸå§‹æ•°æ®ç»“æ„ä½†æŒ‰æ–°é¡ºåºæ’åˆ—
    sorted_data = {"papers": {}}
    for title in paper_titles_ordered:
        sorted_data["papers"][title] = data["papers"][title]
    
    # ä¿å­˜æ’åºåçš„æ•°æ®åˆ°åŸå§‹æ–‡ä»¶
    with open(file_path, 'w', encoding='utf-8') as file:
        json.dump(sorted_data, file, indent=2, ensure_ascii=False)
    print(f"æ’åºåçš„æ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
    
    print(f"\nå¼€å§‹æ›´æ–° {len(paper_titles_ordered)} ç¯‡è®ºæ–‡çš„å¼•ç”¨æ•°...")
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # é€ä¸ªæ›´æ–°å¼•ç”¨æ•°å¹¶ç«‹å³ä¿å­˜
    updated_papers = []
    skipped_papers = []
    
    for index, title in enumerate(paper_titles_ordered):
        print(f"\nå¤„ç†è®ºæ–‡ {index+1}/{len(paper_titles_ordered)}")
        success, result = update_paper_citation(title, sorted_data, file_path)
        
        if success:
            updated_papers.append({
                "title": title,
                "old": sorted_data["papers"][title]["citations"] - result,
                "new": sorted_data["papers"][title]["citations"],
                "change": f"+{result}" if result > 0 else str(result),
                "updated_time": sorted_data["papers"][title]["last_updated"]
            })
        else:
            skipped_papers.append({
                "title": title,
                "reason": result
            })
    
    # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
    print("\n" + "="*80)
    print("æ›´æ–°æ‘˜è¦æŠ¥å‘Š")
    print("="*80)
    print(f"æ€»è®ºæ–‡æ•°: {len(paper_titles_ordered)}")
    print(f"æˆåŠŸæ›´æ–°: {len(updated_papers)} ({(len(updated_papers)/len(paper_titles_ordered)*100):.1f}%)")
    print(f"æ›´æ–°å¤±è´¥: {len(skipped_papers)} ({(len(skipped_papers)/len(paper_titles_ordered)*100):.1f}%)")
    
    if updated_papers:
        print("\n" + "="*80)
        print("æˆåŠŸæ›´æ–°çš„è®ºæ–‡")
        print("="*80)
        for paper in updated_papers:
            print(f"'{paper['title']}'")
            print(f"  å¼•ç”¨å˜åŒ–: {paper['old']} â†’ {paper['new']} ({paper['change']})")
            print(f"  æ›´æ–°æ—¶é—´: {paper['updated_time']}")
    
    if skipped_papers:
        print("\n" + "="*80)
        print("æœªèƒ½æ›´æ–°çš„è®ºæ–‡")
        print("="*80)
        for paper in skipped_papers:
            print(f"'{paper['title']}'")
            print(f"  åŸå› : {paper['reason']}")
    
    print("\n" + "="*80)
    print(f"æ›´æ–°å®Œæˆ! æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
    print(f"æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)

if __name__ == "__main__":
    # æŒ‡å®šcitations.jsonæ–‡ä»¶çš„è·¯å¾„
    file_path = "citations.json"
    
    # è¿è¡Œæ›´æ–°å‡½æ•°
    update_citations_file(file_path)