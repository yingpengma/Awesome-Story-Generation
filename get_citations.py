import json
import requests
import time
from datetime import datetime

def batch_get_citations(paper_titles, original_data, batch_size=10):
    """æ‰¹é‡è·å–å¤šç¯‡è®ºæ–‡çš„å¼•ç”¨æ•°ï¼Œä½¿ç”¨æ­£ç¡®çš„APIæ ¼å¼"""
    base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
    headers = {
        "Content-Type": "application/json",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    # å­˜å‚¨ç»“æœçš„å­—å…¸
    results = {}
    not_found_info = {}  # å­˜å‚¨æœªæ‰¾åˆ°è®ºæ–‡çš„è¯¦ç»†åŸå› 
    
    # å°†è®ºæ–‡æ ‡é¢˜åˆ†æ‰¹å¤„ç†
    batches = [paper_titles[i:i+batch_size] for i in range(0, len(paper_titles), batch_size)]
    
    print("\n" + "="*80)
    print("å¼€å§‹æ‰¹é‡æŸ¥è¯¢è¿‡ç¨‹")
    print("="*80)
    
    for batch_index, batch in enumerate(batches):
        print(f"\næ‰¹æ¬¡ {batch_index+1}/{len(batches)} - å¤„ç† {len(batch)} ç¯‡è®ºæ–‡:")
        
        # é€ä¸ªå¤„ç†æ¯ç¯‡è®ºæ–‡è€Œä¸æ˜¯ä½¿ç”¨batchæ¥å£
        for i, title in enumerate(batch):
            print(f"\nè®ºæ–‡ {batch_index*batch_size + i + 1}: '{title}'")
            old_citation = original_data["papers"][title]["citations"]
            
            try:
                # ä½¿ç”¨searchæ¥å£æŸ¥è¯¢è®ºæ–‡
                params = {
                    "query": title,
                    "fields": "title,citationCount,url",
                    "limit": 5  # è·å–å‡ ä¸ªåŒ¹é…ç»“æœä»¥æé«˜æ‰¾åˆ°çš„æ¦‚ç‡
                }
                
                response = requests.get(base_url, params=params, headers=headers)
                response.raise_for_status()
                data = response.json()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœ
                if not data.get("data") or len(data["data"]) == 0:
                    reason = "APIè¿”å›ç©ºç»“æœ"
                    not_found_info[title] = reason
                    print(f"  âŒ æœªæ‰¾åˆ°: {reason}")
                    continue
                
                # å°è¯•ä»ç»“æœä¸­æ‰¾åˆ°å®Œå…¨åŒ¹é…çš„è®ºæ–‡æ ‡é¢˜
                found = False
                for paper in data["data"]:
                    if 'title' not in paper or 'citationCount' not in paper:
                        continue
                    
                    # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦å®Œå…¨åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
                    if paper['title'].lower() == title.lower():
                        new_citation = paper['citationCount']
                        results[title] = new_citation
                        
                        # æ˜¾ç¤ºå¼•ç”¨å˜åŒ–
                        change = new_citation - old_citation
                        change_symbol = "+" if change > 0 else ""
                        
                        print(f"  âœ… æ›´æ–°æˆåŠŸ: {old_citation} â†’ {new_citation} ({change_symbol}{change})")
                        if 'url' in paper and paper['url']:
                            print(f"  ğŸ“„ è®ºæ–‡é“¾æ¥: {paper['url']}")
                        found = True
                        break
                
                if not found:
                    reason = f"æ ‡é¢˜ä¸å®Œå…¨åŒ¹é…: APIè¿”å›çš„æ ‡é¢˜ä¸æŸ¥è¯¢ä¸åŒ¹é…"
                    not_found_info[title] = reason
                    print(f"  âŒ æœªæ‰¾åˆ°: {reason}")
                    print(f"  ğŸ“ å¤‡æ³¨: æ‰¾åˆ°äº†{len(data['data'])}ä¸ªç»“æœï¼Œä½†æ²¡æœ‰æ ‡é¢˜å®Œå…¨åŒ¹é…çš„")
                
                # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé€Ÿç‡é™åˆ¶
                # time.sleep(1)
                
            except requests.exceptions.RequestException as e:
                error_msg = f"APIè¯·æ±‚é”™è¯¯: {str(e)}"
                print(f"  âŒ {error_msg}")
                not_found_info[title] = error_msg
                # time.sleep(3)  # å‡ºé”™åç­‰å¾…æ›´é•¿æ—¶é—´
            
        # æ¯æ‰¹æ¬¡ä¹‹é—´æ·»åŠ é¢å¤–å»¶è¿Ÿ
        if batch_index < len(batches) - 1:
            print(f"\nç­‰å¾…5ç§’ä»¥é¿å…APIé™åˆ¶...")
            # time.sleep(5)
    
    return results, not_found_info

def update_citations_file(file_path):
    """æ›´æ–°citations.jsonæ–‡ä»¶ä¸­çš„å¼•ç”¨æ•°å¹¶è¯¦ç»†è®°å½•æ•´ä¸ªè¿‡ç¨‹"""
    # è¯»å–ç°æœ‰çš„citations.jsonæ–‡ä»¶
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            data = json.load(file)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"è¯»å–æ–‡ä»¶é”™è¯¯: {e}")
        return
    
    # å¯¹è®ºæ–‡æ ‡é¢˜è¿›è¡Œæ’åºï¼Œå…ˆæŒ‰å¼•ç”¨é‡å‡åºï¼Œç›¸åŒå¼•ç”¨é‡æŒ‰æ›´æ–°æ—¥æœŸå‡åº
    paper_info = []
    for title, info in data["papers"].items():
        # å¯¹äºæ²¡æœ‰last_updatedå­—æ®µçš„è®ºæ–‡ï¼Œè®¾ç½®ä¸€ä¸ªé»˜è®¤å€¼
        last_updated = info.get("last_updated", "1970-01-01-00:00:00")
        paper_info.append({
            "title": title,
            "citations": info["citations"],
            "last_updated": last_updated
        })
    
    # æŒ‰å¼•ç”¨é‡å’Œæ›´æ–°æ—¥æœŸæ’åº
    sorted_papers = sorted(paper_info, key=lambda x: (x["citations"], x["last_updated"]))
    
    # æå–æ’åºåçš„æ ‡é¢˜åˆ—è¡¨
    paper_titles_ordered = [paper["title"] for paper in sorted_papers]
    
    # è¾“å‡ºæ’åºä¿¡æ¯
    print("\n" + "="*80)
    print("è®ºæ–‡æ’åºä¿¡æ¯ï¼ˆæŒ‰å¼•ç”¨é‡å‡åºï¼Œç›¸åŒå¼•ç”¨é‡æŒ‰æ›´æ–°æ—¥æœŸå‡åºï¼‰")
    print("="*80)
    for i, paper in enumerate(sorted_papers):
        print(f"{i+1}. '{paper['title']}' - å¼•ç”¨: {paper['citations']}, æœ€åæ›´æ–°: {paper['last_updated']}")
    
    # å°†æ’åºç»“æœä¿å­˜åˆ°åŸå§‹æ–‡ä»¶
    print("\n" + "="*80)
    print("æ’åºå®Œæˆï¼Œä¿å­˜æ’åºåçš„æ•°æ®åˆ°åŸå§‹æ–‡ä»¶")
    print("="*80)
    
    # åˆ›å»ºä¸€ä¸ªæ–°çš„æœ‰åºå­—å…¸ï¼Œä¿ç•™åŸå§‹æ•°æ®ç»“æ„ä½†æŒ‰æ–°é¡ºåºæ’åˆ—
    sorted_data = {"papers": {}}
    for title in paper_titles_ordered:
        sorted_data["papers"][title] = data["papers"][title]
    
    # ä¿å­˜æ’åºåçš„æ•°æ®åˆ°åŸå§‹æ–‡ä»¶
    with open(file_path, 'w', encoding='utf-8') as file:
        json.dump(sorted_data, file, indent=2, ensure_ascii=False)
    print(f"æ’åºåçš„æ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
    
    print(f"\nå¼€å§‹æ›´æ–° {len(paper_titles_ordered)} ç¯‡è®ºæ–‡çš„å¼•ç”¨æ•°...")
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ‰¹é‡è·å–å¼•ç”¨æ•°
    citation_results, not_found_info = batch_get_citations(paper_titles_ordered, sorted_data)
    
    # æ›´æ–°æ•°æ®å¹¶åˆ›å»ºæŠ¥å‘Š
    update_count = 0
    updated_papers = []
    skipped_papers = []
    
    for title in paper_titles_ordered:
        if title in citation_results:
            old_citation = sorted_data["papers"][title]["citations"]
            new_citation = citation_results[title]
            
            # åªåœ¨æ‰¾åˆ°äº†æ–°å¼•ç”¨æ—¶æ‰æ›´æ–°æ•°æ®å’Œæ—¶é—´æˆ³
            sorted_data["papers"][title]["citations"] = new_citation
            
            # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ—¶é—´æ ¼å¼ YYYY-MM-DD-HH:MM:SS
            current_time = datetime.now().strftime("%Y-%m-%d-%H:%M:%S")
            sorted_data["papers"][title]["last_updated"] = current_time
            
            update_count += 1
            
            # è®°å½•æ›´æ–°ä¿¡æ¯
            change = new_citation - old_citation
            change_str = f"+{change}" if change > 0 else str(change)
            updated_papers.append({
                "title": title, 
                "old": old_citation, 
                "new": new_citation, 
                "change": change_str,
                "updated_time": current_time
            })
        else:
            # è®°å½•æœªæ›´æ–°ä¿¡æ¯ï¼Œä½†ä¸ä¿®æ”¹åŸæ•°æ®
            skipped_papers.append({
                "title": title,
                "reason": not_found_info.get(title, "æœªçŸ¥åŸå› ")
            })
    
    # å†æ¬¡ä¿å­˜æ›´æ–°åçš„æ•°æ®
    with open(file_path, 'w', encoding='utf-8') as file:
        json.dump(sorted_data, file, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºè¯¦ç»†æŠ¥å‘Š
    print("\n" + "="*80)
    print("æ›´æ–°æ‘˜è¦æŠ¥å‘Š")
    print("="*80)
    print(f"æ€»è®ºæ–‡æ•°: {len(paper_titles_ordered)}")
    print(f"æˆåŠŸæ›´æ–°: {update_count} ({(update_count/len(paper_titles_ordered)*100):.1f}%)")
    print(f"æ›´æ–°å¤±è´¥: {len(paper_titles_ordered) - update_count} ({((len(paper_titles_ordered) - update_count)/len(paper_titles_ordered)*100):.1f}%)")
    
    if updated_papers:
        print("\n" + "="*80)
        print("æˆåŠŸæ›´æ–°çš„è®ºæ–‡")
        print("="*80)
        for paper in updated_papers:
            print(f"'{paper['title']}'")
            print(f"  å¼•ç”¨å˜åŒ–: {paper['old']} â†’ {paper['new']} ({paper['change']})")
            print(f"  æ›´æ–°æ—¶é—´: {paper['updated_time']}")
    
    if skipped_papers:
        print("\n" + "="*80)
        print("æœªèƒ½æ›´æ–°çš„è®ºæ–‡")
        print("="*80)
        for paper in skipped_papers:
            print(f"'{paper['title']}'")
            print(f"  åŸå› : {paper['reason']}")
    
    print("\n" + "="*80)
    print(f"æ›´æ–°å®Œæˆ! æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
    print(f"æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)

if __name__ == "__main__":
    # æŒ‡å®šcitations.jsonæ–‡ä»¶çš„è·¯å¾„
    file_path = "citations.json"
    
    # è¿è¡Œæ›´æ–°å‡½æ•°
    update_citations_file(file_path)